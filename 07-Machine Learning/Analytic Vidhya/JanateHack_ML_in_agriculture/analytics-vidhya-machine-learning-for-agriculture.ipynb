{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This is the repeat hackathon of the Analytics Vidhya- Last Man standing Competition.\n#### Earlier Competition link :https://datahack.analyticsvidhya.com/contest/last-man-standing/#About"},{"metadata":{},"cell_type":"markdown","source":"### Rank 1 solution of earlier competition:https://github.com/bishwarup307/AV_Last_Man_Standing\n \n#### Approach :\n* My basic model is an xgboost with multi:softprob like most of you which took me to ~ 85% in public LB. I saved my OOF predictions to pit them against the ground truth to improvise my next level. However, I didn’t find any more potential signal present in the data that could give me an accuracy lift. But very interestingly, what I found is multiple patterns which took me to 96.4% on the LB. Here are the most significant patterns:\n \n* In the data (after binding the train and test), there are contiguous blocks of Estimated Insect Count with max difference of 1.\n \n* Inside each such contiguous blocks there are sublocks first with Crop_Type = 0 and then Crop_Type = 1.\n \n* Inside subblocks there are mini-blocks with Soil_Type = 0 and then Soil_Type = 1\n\n* Inside each mini blocks there is a steady monotonically non decreasing sequence of response values that holds for 100% of the data.\n \n* Since the pattern is deterministic and not probabilistic, i chose to write my own algorithm to modify my model outputs with that instead of engineering features that I can feed to my model. (that also would have given me the lift as Rohan confirms). - This alone takes me to 94.2%.\n \n* Another critical pattern is that- inside each miniblock & response value combination there is steady non-decreasing pattern of Number_Doses_Week which holds for 99.32% of the data - so this can also be treated as more or less deterministic. This helped me to correct the transition entries from 0 to 1 as the pattern only holds for the response value 0 and not 1 and 2.\n \n* After I post processed my outputs from step 5 with step 6 algorithm (which I also written separately)- it took me to 96.4%.\n \n* There were some more patterns in the data that I could mine I think but didn’t get much time to work on those.\n \n* I used R for my XGBOOST and python (Keras) for my NN(~84.2%)."},{"metadata":{},"cell_type":"markdown","source":"### Rank 2 solution of earlier competition :https://github.com/vopani/AnalyticsVidhya_LastManStanding/blob/master/model.R\n\n#### Approach :\nI particularly enjoyed creating the features and seeing it steadily improve the CV and LB. I found the time-series pattern pretty much straight away and from there, it was only uphill.\n\nThis model scored 0.9604 on the public LB and was ranked 2nd."},{"metadata":{},"cell_type":"markdown","source":"### Great dicussion on the problem statement:https://discuss.analyticsvidhya.com/t/last-man-standing-reveal-your-approach/7208\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}