{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackabuse.com/python-for-nlp-introduction-to-the-pattern-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I/PRP/B-NP/O/NP-SBJ-1/i drove/VBD/B-VP/O/VP-1/drive my/PRP$/B-NP/O/NP-OBJ-1/my car/NN/I-NP/O/NP-OBJ-1/car to/TO/O/O/O/to the/DT/B-NP/O/O/the hospital/NN/I-NP/O/O/hospital yesterday/NN/I-NP/O/O/yesterday\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "\n",
    "print(parse('I drove my car to the hospital yesterday', relations=True, lemmata=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pluralizing and Singularizing the Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves\n",
      "theife\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "\n",
    "print(pluralize('leaf'))\n",
    "print(singularize('theives'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Adjective to Comparative and Superlative Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import comparative, superlative\n",
    "\n",
    "print(comparative('good'))\n",
    "print(superlative('good'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'goes'), ('goes', 'to'), ('to', 'hospital')]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import ngrams\n",
    "print(ngrams(\"He goes to hospital\", n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75, 0.8)\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import sentiment\n",
    "\n",
    "print(sentiment(\"This is an excellent movie to watch. I really love it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parse, Sentence\n",
    "from pattern.en import modality\n",
    "\n",
    "text = \"Paris is the capital of France\"\n",
    "sent = parse(text, lemmata=True)\n",
    "sent = Sentence(sent)\n",
    "\n",
    "print(modality(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "text = \"I think we can complete this task\"\n",
    "sent = parse(text, lemmata=True)\n",
    "sent = Sentence(sent)\n",
    "\n",
    "print(modality(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 0.6459209419680404), ('White', 0.2968881412952061), ('Title', 0.03280067283431455), ('Whistle', 0.023549201009251473), ('Chile', 0.0008410428931875525)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "\n",
    "print(suggest(\"Whitle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fracture', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "print(suggest(\"Fracture\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "two hundred and fifty-six point thirty-nine\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import number, numerals\n",
    "\n",
    "print(number(\"one hundred and twenty two\"))\n",
    "print(numerals(256.390, round=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "several bananas, several apples and a pair of mangoes\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import quantify\n",
    "\n",
    "print(quantify(['apple', 'apple', 'apple', 'banana', 'banana', 'banana', 'mango', 'mango']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds of strawberries and a number of peaches\n",
      "thousands of oranges\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import quantify\n",
    "\n",
    "print(quantify({'strawberry': 200, 'peach': 15}))\n",
    "print(quantify('orange', amount=1200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern Library Functions for Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import download\n",
    "\n",
    "page_html = download('https://en.wikipedia.org/wiki/Artificial_intelligence', unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import URL, extension\n",
    "\n",
    "page_url = URL('https://upload.wikimedia.org/wikipedia/commons/f/f1/RougeOr_football.jpg')\n",
    "file = open('football' + extension(page_url.page), 'wb')\n",
    "file.write(page_url.download())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding URLs within Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['www.google.com']\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import find_urls\n",
    "\n",
    "print(find_urls('To search anything, go to www.google.com', unique=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Asynchronous Requests for Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "[Result({'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence', 'title': 'Artificial intelligence - Wikipedia', 'text': '<b>Artificial intelligence</b> (<b>AI</b>), sometimes called machine intelligence, is intelligence <br>\\ndemonstrated by machines, unlike the natural intelligence displayed by humans<br>\\n&nbsp;...'}), Result({'url': 'https://builtin.com/artificial-intelligence', 'title': 'What is Artificial Intelligence? How Does AI Work? | Built In', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) is wide-ranging branch of computer science concerned <br>\\nwith building smart machines capable of performing tasks that typically require&nbsp;...'}), Result({'url': 'https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/', 'title': 'Benefits & Risks of Artificial Intelligence - Future of Life Institute', 'text': 'What is <b>AI</b>? From SIRI to self-driving cars, <b>artificial intelligence</b> (<b>AI</b>) is progressing <br>\\nrapidly. While science fiction often&nbsp;...'}), Result({'url': 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp', 'title': 'Artificial Intelligence (AI) Definition', 'text': '... <b>Artificial intelligence</b> refers to the simulation of human intelligence in machines <br>\\nthat are programmed to think and act like humans.', 'date': 'Mar 13, 2020'}), Result({'url': 'https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html', 'title': 'Artificial Intelligence – What it is and why it matters | SAS', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) makes it possible for machines to learn from experience, <br>\\nadjust to new inputs and perform human-like tasks. Most <b>AI</b> examples that you&nbsp;...'}), Result({'url': 'https://www.britannica.com/technology/artificial-intelligence', 'title': 'artificial intelligence | Definition, Examples, and Applications ...', 'text': '... <b>Artificial intelligence</b>, the ability of a computer or computer-controlled robot to <br>\\nperform tasks commonly associated with intelligent beings.', 'date': 'Aug 11, 2020'}), Result({'url': 'https://www.brookings.edu/research/what-is-artificial-intelligence/', 'title': 'What is artificial intelligence?', 'text': '... <b>Artificial intelligence</b> algorithms are designed to make decisions, often using real-<br>\\ntime data. They are unlike passive machines that are capable&nbsp;...', 'date': 'Oct 4, 2018'}), Result({'url': 'https://www.journals.elsevier.com/artificial-intelligence', 'title': 'Artificial Intelligence - Journal - Elsevier', 'text': 'The journal of <b>Artificial Intelligence</b> (AIJ) welcomes papers on broad aspects of <b>AI</b> <br>\\nthat constitute advances in the overall field including, but not...'}), Result({'url': 'https://www.technologyreview.com/topic/artificial-intelligence/', 'title': 'Artificial intelligence | MIT Technology Review', 'text': '<b>Artificial intelligence</b>. What is <b>AI</b>? It&#39;s the quest to build machines that can reason, <br>\\nlearn, and act intelligently, and it has barely begun. We cover the latest&nbsp;...'}), Result({'url': 'https://www.edx.org/course/artificial-intelligence-ai', 'title': 'Artificial Intelligence (AI) | edX', 'text': 'Learn the fundamentals of <b>Artificial Intelligence</b> (AI), and apply them. Design <br>\\nintelligent agents to solve real-world problems including, search, games, <br>\\n<b>machine</b>&nbsp;...'})]\n",
      "['https://en.wikipedia.org/wiki/Artificial_intelligence', 'https://builtin.com/artificial-intelligence', 'https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/', 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp', 'https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html', 'https://www.britannica.com/technology/artificial-intelligence', 'https://www.brookings.edu/research/what-is-artificial-intelligence/', 'https://www.journals.elsevier.com/artificial-intelligence', 'https://www.technologyreview.com/topic/artificial-intelligence/', 'https://www.edx.org/course/artificial-intelligence-ai']\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import asynchronous, time, Google\n",
    "\n",
    "asyn_req = asynchronous(Google().search, 'artificial intelligence', timeout=4)\n",
    "while not asyn_req.done:\n",
    "    time.sleep(0.1)\n",
    "    print('searching...')\n",
    "\n",
    "print(asyn_req.value)\n",
    "\n",
    "print(find_urls(asyn_req.value, unique=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Search Engine Results with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "<b>Artificial intelligence</b> (<b>AI</b>), sometimes called machine intelligence, is intelligence <br>\n",
      "demonstrated by machines, unlike the natural intelligence displayed by humans<br>\n",
      "&nbsp;...\n",
      "https://builtin.com/artificial-intelligence\n",
      "<b>Artificial intelligence</b> (<b>AI</b>) is wide-ranging branch of computer science concerned <br>\n",
      "with building smart machines capable of performing tasks that typically require&nbsp;...\n",
      "https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/\n",
      "What is <b>AI</b>? From SIRI to self-driving cars, <b>artificial intelligence</b> (<b>AI</b>) is progressing <br>\n",
      "rapidly. While science fiction often&nbsp;...\n",
      "https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp\n",
      "... <b>Artificial intelligence</b> refers to the simulation of human intelligence in machines <br>\n",
      "that are programmed to think and act like humans.\n",
      "https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html\n",
      "<b>Artificial intelligence</b> (<b>AI</b>) makes it possible for machines to learn from experience, <br>\n",
      "adjust to new inputs and perform human-like tasks. Most <b>AI</b> examples that you&nbsp;...\n",
      "https://www.britannica.com/technology/artificial-intelligence\n",
      "... <b>Artificial intelligence</b>, the ability of a computer or computer-controlled robot to <br>\n",
      "perform tasks commonly associated with intelligent beings.\n",
      "https://www.brookings.edu/research/what-is-artificial-intelligence/\n",
      "... <b>Artificial intelligence</b> algorithms are designed to make decisions, often using real-<br>\n",
      "time data. They are unlike passive machines that are capable&nbsp;...\n",
      "https://www.journals.elsevier.com/artificial-intelligence\n",
      "The journal of <b>Artificial Intelligence</b> (AIJ) welcomes papers on broad aspects of <b>AI</b> <br>\n",
      "that constitute advances in the overall field including, but not...\n",
      "https://www.technologyreview.com/topic/artificial-intelligence/\n",
      "<b>Artificial intelligence</b>. What is <b>AI</b>? It&#39;s the quest to build machines that can reason, <br>\n",
      "learn, and act intelligently, and it has barely begun. We cover the latest&nbsp;...\n",
      "https://www.edx.org/course/artificial-intelligence-ai\n",
      "Learn the fundamentals of <b>Artificial Intelligence</b> (AI), and apply them. Design <br>\n",
      "intelligent agents to solve real-world problems including, search, games, <br>\n",
      "<b>machine</b>&nbsp;...\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Google\n",
    "\n",
    "google = Google(license=None)\n",
    "for search_result in google.search('artificial intelligence'):\n",
    "    print(search_result.url)\n",
    "    print(search_result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @BFCXguru: 9 Soft Skills Every #Employee Will Need In The Age Of #ArtificialIntelligence (#AI) \n",
      "\n",
      "https://t.co/lVQd6L5hpM \n",
      "\n",
      "#Technology @bernardmarr @MikeQuindazzi @SpirosMargaris @Ronald_vanLoon @andi_staub @sallyeaves @Fabriziobustama @HaroldSinnott @YuHelenYu @TopCyberNews @Hal_Good\n",
      "A Brief Introduction to Artificial Intelligence #ArtificialIntelligence via https://t.co/qObPsEDIzA https://t.co/dR7MNXvpiI\n",
      "Does Artificial Intelligence Keep Its Promises? #MachineLearning #learning via https://t.co/NEo7LankbK https://t.co/5PHOR8y1Kh\n",
      "RT @BFCXguru: 7 Reason Why #ArtificialIntelligence In #Manufacturing Revolutionizing -\n",
      "\n",
      "https://t.co/QdbjlnUId1 https://t.co/tVAvLXSpWy \n",
      "\n",
      "#AI #Technology #ManufacturingIndustry @Techiexpert @SpirosMargaris @JimMarous @Xbond49 @ahier @BrettKing @missmetaverse @psb_dc @leimer @TopCyberNews\n",
      "“Most strings are random. Most meaningful strings are not.\n",
      "\n",
      "Compression = modeling + coding. Coding is a solved problem.\n",
      "\n",
      "Modeling is provably not solvable.\n",
      "Compression is both an art and an artificial intelligence problem.”\n",
      "\n",
      "https://t.co/jOcOxFU125\n",
      "Researchers have used artificial intelligence to accurately predict loneliness in residents at a senior housing community in San Diego. #TeamEricsson https://t.co/0h9OowQ97k\n",
      "RT @Slate: An artificial intelligence that can truly understand our behavior will be no better than us at dealing with humanity’s challenges. https://t.co/uYnPMhKhCC\n",
      "@Diamond99277560 @SpaceyBeach You cannot offer freewill. Either you have it or you don't. An example is Artificial intelligence at the moment does not have free will. it has multiple choices available to it but not the freedom of choice.\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "index = None\n",
    "for j in range(3):\n",
    "    for tweet in twitter.search('artificial intelligence', start=index, count=3):\n",
    "        print(tweet.text)\n",
    "        index = tweet.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting HTML Data to Plain Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for NLP: Introduction to the TextBlob Library\n",
      "\n",
      "Toggle navigation Stack Abuse\n",
      "\n",
      "* JavaScript\n",
      "* Python\n",
      "* Java\n",
      "* Jobs\n",
      "\n",
      "Python for NLP: Introduction to the TextBlob Library\n",
      "\n",
      "By\n",
      "\n",
      "Usman Malik\n",
      "\n",
      "•0 Comments\n",
      "\n",
      "Introduction\n",
      "\n",
      "This is the seventh article in my series of articles on Python for NLP. In my previous article, I explained how to perform topic modeling using Latent Dirichlet Allocation and Non-Negative Matrix factorization. We used the Scikit-Learn library to perform topic modeling.\n",
      "\n",
      "In this article, we will explore TextBlob, which is another extremely powerful NLP library for Python. TextBlob is built upon NLTK and provides an easy to use interface to the NLTK library. We will see how TextBlob can be used to perform a variety of NLP tasks ranging from parts-of-speech tagging to sentiment analysis, and language translation to text classification.\n",
      "\n",
      "The detailed download instructions for the library can be found at the official link. I would suggest that you install the TextBlob library as well as the sample corpora.\n",
      "\n",
      "Here is the gist of the instructions linked above, but be sure to check the official documentation for more instructions on installing if you need it:\n",
      "\n",
      "$ pip install -U textblob\n",
      "\n",
      "And to install the corpora:\n",
      "\n",
      "$ python -m textblob.download_corpora\n",
      "\n",
      "Let's now see the different functionalities of the TextBlob library.\n",
      "\n",
      "Tokenization\n",
      "\n",
      "Tokenization refers to splitting a large paragraph into sentences or words. Typically, a token refers to a word in a text document. Tokenization is pretty straight forward with TextBlob. All you have to do is import the\n",
      "TextBlob\n",
      "\n",
      "object from the\n",
      "textblob\n",
      "\n",
      "library, pass it the document that you want to tokenize, and then use the\n",
      "sentences\n",
      "\n",
      "and\n",
      "words\n",
      "\n",
      "attributes to get the tokenized sentences and attributes. Let's see this in action:\n",
      "\n",
      "The first step is to import the\n",
      "TextBlob\n",
      "\n",
      "object:\n",
      "\n",
      "from textblob import TextBlob\n",
      "\n",
      "Next, you need to define a string that contains the text of the document. We will create string that contains the first paragraph of the Wikipedia article on artificial intelligence.\n",
      "\n",
      "document = (\"In computer science, artificial intelligence (AI), \\\n",
      "sometimes called machine intelligence, is intelligence \\\n",
      "demonstrated by machines, in contrast to the natural intelligence \\\n",
      "displayed by humans and animals. Computer science defines AI \\\n",
      "research as the study of \\\"intelligent agents\\\": any device that \\\n",
      "perceives its environment and takes actions that maximize its\\\n",
      "chance of successfully achieving its goals.[1] Colloquially,\\\n",
      "the term \\\"artificial intelligence\\\" is used to describe machines\\\n",
      "that mimic \\\"cognitive\\\" functions that humans associate with other\\\n",
      "human minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")\n",
      "\n",
      "The next step is to pass this document as a parameter to the\n",
      "TextBlob\n",
      "\n",
      "class. The returned object can then be used to tokenize the document to words and sentences.\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "\n",
      "Now to get the tokenized sentences, we can use the\n",
      "sentences\n",
      "\n",
      "attribute:\n",
      "\n",
      "document_sentence = text_blob_object.sentences\n",
      "\n",
      "print(document_sentence)\n",
      "print(len(document_sentence))\n",
      "\n",
      "In the output, you will see the tokenized sentences along with the number of sentences.\n",
      "\n",
      "[Sentence(\"In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"), Sentence(\"Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\"), Sentence(\"[1] Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".\"), Sentence(\"[2]\")]\n",
      "4\n",
      "\n",
      "Similarly, the\n",
      "words\n",
      "\n",
      "attribute returns the tokenized words in the document.\n",
      "\n",
      "document_words = text_blob_object.words\n",
      "\n",
      "print(document_words)\n",
      "print(len(document_words))\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "['In', 'computer', 'science', 'artificial', 'intelligence', 'AI', 'sometimes', 'called', 'machine', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machines', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals', 'Computer', 'science', 'defines', 'AI', 'research', 'as', 'the', 'study', 'of', 'intelligent', 'agents', 'any', 'device', 'that', 'perceives', 'its', 'environment', 'and', 'takes', 'actions', 'that', 'maximize', 'its', 'chance', 'of', 'successfully', 'achieving', 'its', 'goals', '1', 'Colloquially', 'the', 'term', 'artificial', 'intelligence', 'is', 'used', 'to', 'describe', 'machines', 'that', 'mimic', 'cognitive', 'functions', 'that', 'humans', 'associate', 'with', 'other', 'human', 'minds', 'such', 'as', 'learning', 'and', 'problem', 'solving', '2']\n",
      "84\n",
      "\n",
      "Lemmatization\n",
      "\n",
      "Lemmatization refers to reducing the word to its root form as found in a dictionary.\n",
      "\n",
      "To perform lemmatization via TextBlob, you have to use the\n",
      "Word\n",
      "\n",
      "object from the\n",
      "textblob\n",
      "\n",
      "library, pass it the word that you want to lemmatize and then call the\n",
      "lemmatize\n",
      "\n",
      "method.\n",
      "\n",
      "from textblob import Word\n",
      "\n",
      "word1 = Word(\"apples\")\n",
      "print(\"apples:\", word1.lemmatize())\n",
      "\n",
      "word2 = Word(\"media\")\n",
      "print(\"media:\", word2.lemmatize())\n",
      "\n",
      "word3 = Word(\"greater\")\n",
      "print(\"greater:\", word3.lemmatize(\"a\"))\n",
      "\n",
      "In the script above, we perform lemmatization on the words \"apples\", \"media\", and \"greater\". In the output, you will see the words \"apple\", (which is singular for the apple), \"medium\" (which is singular for the medium) and \"great\" (which is the positive degree for the word greater). Notice that for the word greater, we pass \"a\" as a parameter to the\n",
      "lemmatize\n",
      "\n",
      "method. This specifically tells the method that the word should be treated as an adjective. By default, the words are treated as nouns by the\n",
      "lemmatize()\n",
      "\n",
      "method. The complete list for the parts of speech components is as follows:\n",
      "\n",
      "ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
      "\n",
      "Parts of Speech (POS) Tagging\n",
      "\n",
      "Like the spaCy and NLTK libraries, the TextBlob library also contains functionalities for the POS tagging.\n",
      "\n",
      "To find POS tags for the words in a document, all you have to do is use the\n",
      "tags\n",
      "\n",
      "attribute as shown below:\n",
      "\n",
      "for word, pos in text_blob_object.tags:\n",
      "print(word + \" => \" + pos)\n",
      "\n",
      "In the script above, print the tags for all the words in the first paragraph of the Wikipedia article on Artificial Intelligence. The output of the script above looks like this:\n",
      "\n",
      "```\n",
      "In => IN\n",
      "computer => NN\n",
      "science => NN\n",
      "artificial => JJ\n",
      "intelligence => NN\n",
      "AI => NNP\n",
      "sometimes => RB\n",
      "called => VBD\n",
      "machine => NN\n",
      "intelligence => NN\n",
      "is => VBZ\n",
      "intelligence => NN\n",
      "demonstrated => VBN\n",
      "by => IN\n",
      "machines => NNS\n",
      "in => IN\n",
      "contrast => NN\n",
      "to => TO\n",
      "the => DT\n",
      "natural => JJ\n",
      "intelligence => NN\n",
      "displayed => VBN\n",
      "by => IN\n",
      "humans => NNS\n",
      "and => CC\n",
      "animals => NNS\n",
      "Computer => NNP\n",
      "science => NN\n",
      "defines => NNS\n",
      "AI => NNP\n",
      "research => NN\n",
      "as => IN\n",
      "the => DT\n",
      "study => NN\n",
      "of => IN\n",
      "intelligent => JJ\n",
      "agents => NNS\n",
      "any => DT\n",
      "device => NN\n",
      "that => WDT\n",
      "perceives => VBZ\n",
      "its => PRP$\n",
      "environment => NN\n",
      "and => CC\n",
      "takes => VBZ\n",
      "actions => NNS\n",
      "that => IN\n",
      "maximize => VB\n",
      "its => PRP$\n",
      "chance => NN\n",
      "of => IN\n",
      "successfully => RB\n",
      "achieving => VBG\n",
      "its => PRP$\n",
      "goals => NNS\n",
      "[ => RB\n",
      "1 => CD\n",
      "] => NNP\n",
      "Colloquially => NNP\n",
      "the => DT\n",
      "term => NN\n",
      "artificial => JJ\n",
      "intelligence => NN\n",
      "is => VBZ\n",
      "used => VBN\n",
      "to => TO\n",
      "describe => VB\n",
      "machines => NNS\n",
      "that => IN\n",
      "mimic => JJ\n",
      "cognitive => JJ\n",
      "functions => NNS\n",
      "that => WDT\n",
      "humans => NNS\n",
      "associate => VBP\n",
      "with => IN\n",
      "other => JJ\n",
      "human => JJ\n",
      "minds => NNS\n",
      "such => JJ\n",
      "as => IN\n",
      "learning => VBG\n",
      "and => CC\n",
      "problem => NN\n",
      "solving => NN\n",
      "[ => RB\n",
      "2 => CD\n",
      "] => NNS\n",
      "```\n",
      "\n",
      "The POS tags have been printed in the abbreviation form. To see the full form of each abbreviation, please consult this link.\n",
      "\n",
      "Convert Text to Singular and Plural\n",
      "\n",
      "TextBlob also allows you to convert text into a plural or singular form using the\n",
      "pluralize\n",
      "\n",
      "and\n",
      "singularize\n",
      "\n",
      "methods, respectively. Look at the following example:\n",
      "\n",
      "text = (\"Football is a good game. It has many health benefit\")\n",
      "text_blob_object = TextBlob(text)\n",
      "print(text_blob_object.words.pluralize())\n",
      "\n",
      "In the output, you will see the plural of all the words:\n",
      "\n",
      "['Footballs', 'iss', 'some', 'goods', 'games', 'Its', 'hass', 'manies', 'healths', 'benefits']\n",
      "\n",
      "Similarly, to singularize words you can use\n",
      "singularize\n",
      "\n",
      "method as follows:\n",
      "\n",
      "text = (\"Footballs is a goods games. Its has many healths benefits\")\n",
      "\n",
      "text_blob_object = TextBlob(text)\n",
      "print(text_blob_object.words.singularize())\n",
      "\n",
      "The output of the script above looks like this:\n",
      "\n",
      "['Football', 'is', 'a', 'good', 'game', 'It', 'ha', 'many', 'health', 'benefit']\n",
      "\n",
      "Noun Phrase Extraction\n",
      "\n",
      "Noun phrase extraction, as the name suggests, refers to extracting phrases that contain nouns. Let's find all the noun phrases in the first paragraph of the Wikipedia article on artificial intelligence that we used earlier.\n",
      "\n",
      "To find noun phrases, you simply have to use the\n",
      "noun_phrase\n",
      "\n",
      "attributes on the\n",
      "TextBlob\n",
      "\n",
      "object. Look at the following example:\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "for noun_phrase in text_blob_object.noun_phrases:\n",
      "print(noun_phrase)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "computer science\n",
      "artificial intelligence\n",
      "ai\n",
      "machine intelligence\n",
      "natural intelligence\n",
      "computer\n",
      "science defines\n",
      "ai\n",
      "intelligent agents\n",
      "colloquially\n",
      "artificial intelligence\n",
      "describe machines\n",
      "human minds\n",
      "\n",
      "You can see all the noun phrases in our document.\n",
      "\n",
      "Getting Words and Phrase Counts\n",
      "\n",
      "In a previous section, we used Python's built-in\n",
      "len\n",
      "\n",
      "method to count the number of sentences, words and noun-phrases returned by the\n",
      "TextBlob\n",
      "\n",
      "object. We can use TextBlob's built-in methods for the same purpose.\n",
      "\n",
      "To find the frequency of occurrence of a particular word, we have to pass the name of the word as the index to the\n",
      "word_counts\n",
      "\n",
      "list of the\n",
      "TextBlob\n",
      "\n",
      "object.\n",
      "\n",
      "In the following example, we will count the number of instances of the word \"intelligence\" in the first paragraph of the Wikipedia article on Artificial Intelligence.\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "text_blob_object.word_counts['intelligence']\n",
      "\n",
      "Another way is to simply call the\n",
      "count\n",
      "\n",
      "method on the\n",
      "words\n",
      "\n",
      "attribute, and pass the name of the word whose frequency of occurrence is to be found as shown below:\n",
      "\n",
      "text_blob_object.words.count('intelligence')\n",
      "\n",
      "It is important to mention that by default the search is not case-sensitive. If you want your search to be case sensitive, you need to pass\n",
      "True\n",
      "\n",
      "as the value for the\n",
      "case_sensitive\n",
      "\n",
      "parameter, as shown below:\n",
      "\n",
      "text_blob_object.words.count('intelligence', case_sensitive=True)\n",
      "\n",
      "Like word counts, noun phrases can also be counted in the same way. The following example finds the phrase \"artificial intelligence\" in the paragraph.\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "text_blob_object.noun_phrases.count('artificial intelligence')\n",
      "\n",
      "In the output, you will see 2.\n",
      "\n",
      "Converting to Upper and Lowercase\n",
      "\n",
      "TextBlob objects are very similar to strings. You can convert them to upper case or lower case, change their values, and concatenate them together as well. In the following script, we convert the text from the TextBlob object to upper case:\n",
      "\n",
      "text = \"I love to watch football, but I have never played it\"\n",
      "text_blob_object = TextBlob(text)\n",
      "\n",
      "print(text_blob_object.upper())\n",
      "\n",
      "In the output, you will the string in the upper case:\n",
      "\n",
      "I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\n",
      "\n",
      "Similarly to convert the text to lowercase, we can use the\n",
      "lower()\n",
      "\n",
      "method as shown below:\n",
      "\n",
      "text = \"I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\"\n",
      "text_blob_object = TextBlob(text)\n",
      "\n",
      "print(text_blob_object.lower())\n",
      "\n",
      "Finding N-Grams\n",
      "\n",
      "N-Grams refer to n combination of words in a sentence. For instance, for a sentence \"I love watching football\", some 2-grams would be (I love), (love watching) and (watching football). N-Grams can play a crucial role in text classification.\n",
      "\n",
      "In TextBlob, N-grams can be found by passing the number of N-Grams to the\n",
      "ngrams\n",
      "\n",
      "method of the\n",
      "TextBlob\n",
      "\n",
      "object. Look at the following example:\n",
      "\n",
      "text = \"I love to watch football, but I have never played it\"\n",
      "text_blob_object = TextBlob(text)\n",
      "for ngram in text_blob_object.ngrams(2):\n",
      "print(ngram)\n",
      "\n",
      "The output of the script looks like this:\n",
      "\n",
      "['I', 'love']\n",
      "['love', 'to']\n",
      "['to', 'watch']\n",
      "['watch', 'football']\n",
      "['football', 'but']\n",
      "['but', 'I']\n",
      "['I', 'have']\n",
      "['have', 'never']\n",
      "['never', 'played']\n",
      "['played', 'it']\n",
      "\n",
      "This is especially helpful when training language models or doing any type of text prediction.\n",
      "\n",
      "Spelling Corrections\n",
      "\n",
      "Spelling correction is one of the unique functionalities of the TextBlob library. With the\n",
      "correct\n",
      "\n",
      "method of the\n",
      "TextBlob\n",
      "\n",
      "object, you can correct all the spelling mistakes in your text. Look at the following example:\n",
      "\n",
      "text = \"I love to watchf footbal, but I have neter played it\"\n",
      "text_blob_object = TextBlob(text)\n",
      "\n",
      "print(text_blob_object.correct())\n",
      "\n",
      "In the script above we made three spelling mistakes: \"watchf\" instead of \"watch\", \"footbal\" instead of \"football\", \"neter\" instead of \"never\". In the output, you will see that these mistakes have been corrected by TextBlob, as shown below:\n",
      "\n",
      "I love to watch football, but I have never played it\n",
      "\n",
      "Language Translation\n",
      "\n",
      "One of the most powerful capabilities of the TextBlob library is to translate from one language to another. On the backend, the TextBlob language translator uses the Google Translate API\n",
      "\n",
      "To translate from one language to another, you simply have to pass the text to the\n",
      "TextBlob\n",
      "\n",
      "object and then call the\n",
      "translate\n",
      "\n",
      "method on the object. The language code for the language that you want your text to be translated to is passed as a parameter to the method. Let's take a look at an example:\n",
      "\n",
      "text_blob_object_french = TextBlob(u'Salut comment allez-vous?')\n",
      "print(text_blob_object_french.translate(to='en'))\n",
      "\n",
      "In the script above, we pass a sentence in the French language to the\n",
      "TextBlob\n",
      "\n",
      "object. Next, we call the\n",
      "translate\n",
      "\n",
      "method on the object and pass language code\n",
      "en\n",
      "\n",
      "to the\n",
      "to\n",
      "\n",
      "parameter. The language code\n",
      "en\n",
      "\n",
      "corresponds to the English language. In the output, you will see the translation of the French sentence as shown below:\n",
      "\n",
      "Hi, how are you?\n",
      "\n",
      "Let's take another example where we will translate from Arabic to English:\n",
      "\n",
      "text_blob_object_arabic = TextBlob(u'مرحبا كيف حالك؟')\n",
      "print(text_blob_object_arabic.translate(to='en'))\n",
      "\n",
      "Output:\n",
      "\n",
      "Hi, how are you?\n",
      "\n",
      "Finally, using the\n",
      "detect_language\n",
      "\n",
      "method, you can also detect the language of the sentence. Look at the following script:\n",
      "\n",
      "text_blob_object = TextBlob(u'Hola como estas?')\n",
      "print(text_blob_object.detect_language())\n",
      "\n",
      "In the output, you will see\n",
      "es\n",
      "\n",
      ", which stands for the Spanish language.\n",
      "\n",
      "The language code for all the languages can be found at this link.\n",
      "\n",
      "Text Classification\n",
      "\n",
      "TextBlob also provides basic text classification capabilities. Though, I would not recommend TextBlob for text classification owing to its limited capabilities, however, if you have a really limited data and you want to quickly develop a very basic text classification model, then you may use TextBlob. For advanced models, I would recommend machine learning libraries such as Scikit-Learn or Tensorflow.\n",
      "\n",
      "Let's see how we can perform text classification with TextBlob. The first thing we need is a training dataset and test data. The classification model will be trained on the training dataset and will be evaluated on the test dataset.\n",
      "\n",
      "Suppose we have the following training and test data:\n",
      "\n",
      "train_data = [\n",
      "('This is an excellent movie', 'pos'),\n",
      "('The move was fantastic I like it', 'pos'),\n",
      "('You should watch it, it is brilliant', 'pos'),\n",
      "('Exceptionally good', 'pos'),\n",
      "(\"Wonderfully directed and executed. I like it\", 'pos'),\n",
      "('It was very boring', 'neg'),\n",
      "('I did not like the movie', 'neg'),\n",
      "(\"The movie was horrible\", 'neg'),\n",
      "('I will not recommend', 'neg'),\n",
      "('The acting is pathetic', 'neg')\n",
      "]\n",
      "test_data = [\n",
      "('Its a fantastic series', 'pos'),\n",
      "('Never watched such a brillent movie', 'pos'),\n",
      "(\"horrible acting\", 'neg'),\n",
      "(\"It is a Wonderful movie\", 'pos'),\n",
      "('waste of money', 'neg'),\n",
      "(\"pathetic picture\", 'neg')\n",
      "]\n",
      "\n",
      "The dataset contains some dummy reviews about movies. You can see our training and test datasets consist of lists of tuples where the first element of the tuple is the text or a sentence while the second member of the tuple is the corresponding review or sentiment of the text.\n",
      "\n",
      "We will train our dataset on the\n",
      "train_data\n",
      "\n",
      "and will evaluate it on the\n",
      "test_data\n",
      "\n",
      ". To do so, we will use the\n",
      "NaiveBayesClassifier\n",
      "\n",
      "class from the\n",
      "textblob.classifiers\n",
      "\n",
      "library. The following script imports the library:\n",
      "\n",
      "from textblob.classifiers import NaiveBayesClassifier\n",
      "\n",
      "To train the model, we simply have to pass the training data to the constructor of the\n",
      "NaiveBayesClassifier\n",
      "\n",
      "class. The class will return an object trained on the dataset and capable of making predictions on the test set.\n",
      "\n",
      "classifier = NaiveBayesClassifier(train_data)\n",
      "\n",
      "Let's first make a prediction on a single sentence. To do so, we need to call the\n",
      "classify\n",
      "\n",
      "method and pass it the sentence. Look at the following example:\n",
      "\n",
      "print(classifier.classify(\"It is very boring\"))\n",
      "\n",
      "It looks like a negative review. When you execute the above script, you will see\n",
      "neg\n",
      "\n",
      "in the output.\n",
      "\n",
      "Similarly, the following script will return\n",
      "pos\n",
      "\n",
      "since the review is positive.\n",
      "\n",
      "print(classifier.classify(\"It's a fantastic series\"))\n",
      "\n",
      "You can also make a prediction by passing our\n",
      "classifier\n",
      "\n",
      "to the\n",
      "classifier\n",
      "\n",
      "parameter of the\n",
      "TextBlob\n",
      "\n",
      "object. You then have to call the\n",
      "classify\n",
      "\n",
      "method on the\n",
      "TextBlob\n",
      "\n",
      "object to view the prediction.\n",
      "\n",
      "sentence = TextBlob(\"It's a fantastic series.\", classifier=classifier)\n",
      "print(sentence.classify())\n",
      "\n",
      "Finally, to find the accuracy of your algorithm on the test set, call the\n",
      "accuracy\n",
      "\n",
      "method on your classifier and pass it the\n",
      "test_data\n",
      "\n",
      "that we just created. Look at the following script:\n",
      "\n",
      "classifier.accuracy(test_data)\n",
      "\n",
      "In the output, you will see 0.66 which is the accuracy of the algorithm.\n",
      "\n",
      "To find the most important features for the classification, the\n",
      "show_informative_features\n",
      "\n",
      "method can be used. The number of most important features to see is passed as a parameter.\n",
      "\n",
      "classifier.show_informative_features(3)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "Most Informative Features\n",
      "contains(it) = False neg : pos = 2.2 : 1.0\n",
      "contains(is) = True pos : neg = 1.7 : 1.0\n",
      "contains(was) = True neg : pos = 1.7 : 1.0\n",
      "\n",
      "In this section, we tried to find the sentiment of the movie review using text classification. In reality, you don't have to perform text classification to find the sentiment of a sentence in TextBlob. The TextBlob library comes with a built-in sentiment analyzer which we will see in the next section.\n",
      "\n",
      "Sentiment Analysis\n",
      "\n",
      "In this section, we will analyze the sentiment of the public reviews for different foods purchased via Amazon. We will use the TextBlob sentiment analyzer to do so.\n",
      "\n",
      "The dataset can be downloaded from this Kaggle link.\n",
      "\n",
      "As a first step, we need to import the dataset. We will only import the first 20,000 records due to memory constraints. You can import more records if you want. The following script imports the dataset:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "reviews_datasets = pd.read_csv(r'E:\\Datasets\\Reviews.csv')\n",
      "reviews_datasets = reviews_datasets.head(20000)\n",
      "reviews_datasets.dropna()\n",
      "\n",
      "To see how our dataset looks, we will use the\n",
      "head\n",
      "\n",
      "method of the pandas data frame:\n",
      "\n",
      "reviews_datasets.head()\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "From the output, you can see that the text review about the food is contained by the Text column. The score column contains ratings of the user for the particular product with 1 being the lowest and 5 being the highest rating.\n",
      "\n",
      "Let's see the distribution of rating:\n",
      "\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "sns.distplot(reviews_datasets['Score'])\n",
      "\n",
      "You can see that most of the ratings are highly positive i.e. 5. Let's plot the bar plot for the ratings to have a better look at the number of records for each rating.\n",
      "\n",
      "sns.countplot(x='Score', data=reviews_datasets)\n",
      "\n",
      "The output shows that more than half of reviews have 5-star ratings.\n",
      "\n",
      "Let's randomly select a review and find its polarity using TextBlob. Let's take a look at review number 350.\n",
      "\n",
      "reviews_datasets['Text'][350]\n",
      "\n",
      "Output:\n",
      "\n",
      "'These chocolate covered espresso beans are wonderful! The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.'\n",
      "\n",
      "It looks like the review is positive. Let's verify this using the TextBlob library. To find the sentiment, we have to use the\n",
      "sentiment\n",
      "\n",
      "attribute of the\n",
      "TextBlog\n",
      "\n",
      "object. The\n",
      "sentiment\n",
      "\n",
      "object returns a tuple that contains polarity and subjectivity of the review.\n",
      "\n",
      "The value of polarity can be between -1 and 1 where the reviews with negative polarities have negative sentiments while the reviews with positive polarities have positive sentiments.\n",
      "\n",
      "The subjectivity value can be between 0 and 1. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information.\n",
      "\n",
      "Let's find the sentiment of the 350th review.\n",
      "\n",
      "text_blob_object = TextBlob(reviews_datasets['Text'][350])\n",
      "print(text_blob_object.sentiment)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "Sentiment(polarity=0.39666666666666667,subjectivity=0.6616666666666667)\n",
      "\n",
      "The output shows that the review is positive with a high subjectivity.\n",
      "\n",
      "Let's now add a column for sentiment polarity in our dataset. Execute the following script:\n",
      "\n",
      "def find_pol(review):\n",
      "return TextBlob(review).sentiment.polarity\n",
      "\n",
      "reviews_datasets['Sentiment_Polarity'] = reviews_datasets['Text'].apply(find_pol)\n",
      "reviews_datasets.head()\n",
      "\n",
      "Now let's see the distribution of polarity in our dataset. Execute the following script:\n",
      "\n",
      "sns.distplot(reviews_datasets['Sentiment_Polarity'])\n",
      "\n",
      "The output of the script above looks like this:\n",
      "\n",
      "It is evident from the figure above that most of the reviews are positive and have polarity between 0 and 0.5. This is natural since most of the reviews in the dataset have 5-star ratings.\n",
      "\n",
      "Let's now plot the average polarity for each score rating.\n",
      "\n",
      "sns.barplot(x='Score', y='Sentiment_Polarity', data=reviews_datasets)\n",
      "\n",
      "Output:\n",
      "\n",
      "The output clearly shows that the reviews with high rating scores have high positive polarities.\n",
      "\n",
      "Let's now see some of the most negative reviews i.e. the reviews with a polarity value of -1.\n",
      "\n",
      "most_negative = reviews_datasets[reviews_datasets.Sentiment_Polarity == -1].Text.head()\n",
      "print(most_negative)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "545 These chips are nasty. I thought someone had ...\n",
      "1083 All my fault. I thought this would be a carton...\n",
      "1832 Pop Chips are basically a horribly over-priced...\n",
      "2087 I do not consider Gingerbread, Spicy Eggnog, C...\n",
      "2763 This popcorn has alot of hulls I order 4 bags ...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Let's print the value of review number 545.\n",
      "\n",
      "reviews_datasets['Text'][545]\n",
      "\n",
      "In the output, you will see the following review:\n",
      "\n",
      "'These chips are nasty. I thought someone had spilled a drink in the bag, no the chips were just soaked with grease. Nasty!!'\n",
      "\n",
      "The output clearly shows that the review is highly negative.\n",
      "\n",
      "Let's now see some of the most positive reviews. Execute the following script:\n",
      "\n",
      "most_positive = reviews_datasets[reviews_datasets.Sentiment_Polarity == 1].Text.head()\n",
      "print(most_positive)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "106 not what I was expecting in terms of the compa...\n",
      "223 This is an excellent tea. One of the best I h...\n",
      "338 I like a lot of sesame oil and use it in salad...\n",
      "796 My mother and father were the recipient of the...\n",
      "1031 The Kelloggs Muselix are delicious and the del...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Let's see review 106 in detail:\n",
      "\n",
      "reviews_datasets['Text'][106]\n",
      "\n",
      "Output:\n",
      "\n",
      "\"not what I was expecting in terms of the company's reputation for excellent home delivery products\"\n",
      "\n",
      "You can see that though the review was not very positive, it has been assigned a polarity of 1 due to the presence of words like\n",
      "excellent\n",
      "\n",
      "and\n",
      "reputation\n",
      "\n",
      ". It is important to know that sentiment analyzer is not 100% error-proof and might predict wrong sentiment in a few cases, such as the one we just saw.\n",
      "\n",
      "Let's now see review number 223 which also has been marked as positive.\n",
      "\n",
      "reviews_datasets['Text'][223]\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "\"This is an excellent tea. One of the best I have ever had. It is especially great when you prepare it with a samovar.\"\n",
      "\n",
      "The output clearly depicts that the review is highly positive.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Python's TextBlob library is one of the most famous and widely used natural language processing libraries. This article explains several functionalities of the TextBlob library, such as tokenization, stemming, sentiment analysis, text classification and language translation in detail.\n",
      "\n",
      "In the next article I'll go over the Pattern library, which provides a lot of really useful functions for determining attributes about sentences, as well as tools for retrieving data from social networks, Wikipedia, and search engines.\n",
      "\n",
      "python,nlp\n",
      "\n",
      "*\n",
      "*\n",
      "*\n",
      "\n",
      "About Usman Malik\n",
      "\n",
      "Paris (France) Twitter\n",
      "\n",
      "Programmer | Blogger | Data Science Enthusiast | PhD To Be | Arsenal FC for Life\n",
      "\n",
      "Subscribe to our Newsletter\n",
      "\n",
      "Please enable JavaScript to view the comments powered by Disqus.\n",
      "\n",
      "Previous Post\n",
      "\n",
      "Next Post\n",
      "\n",
      "Ad\n",
      "\n",
      "Follow Us\n",
      "\n",
      "Twitter\n",
      "\n",
      "Facebook\n",
      "\n",
      "RSS\n",
      "\n",
      "Data Visualization in Python\n",
      "\n",
      "Understand your data better with visualizations! With over 275+ pages, you'll learn the ins and outs of visualizing data in Python with popular libraries like Matplotlib, Seaborn, Bokeh, and more.\n",
      "\n",
      "Learn more Preview\n",
      "\n",
      "Getting Started with AWS in Node\n",
      "\n",
      "Build the foundation you'll need to provision, deploy, and run Node.js applications in the AWS cloud. Learn Lambda, EC2, S3, and more!\n",
      "Pre-order now for over 30% off!\n",
      "\n",
      "Pre-order now\n",
      "\n",
      "Newsletter\n",
      "\n",
      "Subscribe to our newsletter! Get occassional tutorials, guides, and reviews in your inbox.\n",
      "\n",
      "Ad\n",
      "\n",
      "Want a remote job?\n",
      "\n",
      "More jobs\n",
      "\n",
      "Jobs via\n",
      "HireRemote.io\n",
      "\n",
      "Prepping for an interview?\n",
      "\n",
      "* Improve your skills by solving one coding problem every day\n",
      "\n",
      "* Get the solutions the next morning via email\n",
      "\n",
      "* Practice on actual problems asked by top companies, like:\n",
      "\n",
      "Daily Coding Problem\n",
      "\n",
      "Ad\n",
      "\n",
      "Recent Posts\n",
      "\n",
      "Lazy-Loading Routes with Vue Router with a Progress Bar\n",
      "\n",
      "Writing to Files Using cat Command on Linux\n",
      "\n",
      "Working with Images in Node.js - GraphicsMagick and ImageMagick\n",
      "\n",
      "Tags\n",
      "\n",
      "aialgorithmsamqpangularannouncementsapacheapache commonsapiarduinoartificial intelligence\n",
      "\n",
      "Follow Us\n",
      "\n",
      "Twitter\n",
      "\n",
      "Facebook\n",
      "\n",
      "RSS\n",
      "\n",
      "Copyright © 2020, Stack Abuse. All Rights Reserved.\n",
      "\n",
      "Disclosure•Privacy Policy•Terms of Service\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import URL, plaintext\n",
    "\n",
    "html_content = URL('https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/').download()\n",
    "cleaned_page = plaintext(html_content.decode('utf-8'))\n",
    "print(cleaned_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing PDF Documments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import URL, PDF\n",
    "pdf_doc = URL('http://demo.clab.cs.cmu.edu/NLP/syllabus_f18.pdf').download()\n",
    "#print(PDF(pdf_doc.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearing the Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import cache\n",
    "\n",
    "cache.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\plthi'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
